{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iqiESu0Czs9"
      },
      "source": [
        "# Domain adaptation for MLMs: How to teach a BERT-family about the *movies* domain via MLM finetuning\n",
        "\n",
        "In this notebook we'll perform MLM fine-tuning of a pre-trained BERT-family model `DistilBERT` Specfically, we want to adjust `DistilBERT` weights to favor the *movies* domain.\n",
        "\n",
        "We'll do this by performing MLM training on the IMDB Sentiment Analysis dataset.  The notebook demonstrates the process of preparing data, training, evaluating, and\n",
        "sharing the fine-tuned model, using Hugging Face Transformers, Datasets,\n",
        "and Accelerate libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mURDBvRHCztB"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvvS_iFsCztC"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvOP7VOOCztE"
      },
      "source": [
        "### 1. Create an environment for training and sharing language models within the Hugging Face ecosystem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mjjwQTECztE"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E68VGmoqCztF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSb2crLVK7fQ"
      },
      "source": [
        "### 2. Load the BERT-family model and its tokenizer; then investigate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plo37X72EcvH"
      },
      "source": [
        "Import the `AutoModelForMaskedLM` class from the transformers library. This class is used to load pre-trained models specifically designed for MLM tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xkFc4aEox3"
      },
      "source": [
        "\n",
        "\n",
        "*   Specify the name of the pre-trained model we want. In this case, it's the `DistilBERT model`, a smaller and faster version of `BERT`.\n",
        "*   Call the `AutoModelForMaskedLM.from_pretrained()` method to download and load the pre-trained `DistilBERT` model specified by model_checkpoint. This model is now stored in the model variable and ready for use in MLM, such as filling in missing words in a sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agEs8YEoCztG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDGbGU1VFjv8"
      },
      "source": [
        "\n",
        "\n",
        "*   `model.num_parameters()`: This function call retrieves the total number of trainable parameters in the loaded `DistilBERT` model.\n",
        "*   `/ 1_000_000`: This division converts the parameter count to millions (M) for easier readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weDRwI3eCztG"
      },
      "outputs": [],
      "source": [
        "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
        "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
        "print(f\"'>>> BERT number of parameters: 110M'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8PM6epvF-z0"
      },
      "source": [
        "\n",
        "\n",
        "*   `[MASK]`: This special token `[MASK]` is a placeholder. In MLM, we hide certain words in a sentence and train a model to predict those hidden words. The `[MASK]` token represents the hidden word the model needs to predict.\n",
        "\n",
        "* We'll use this input to probe for the model's current (before finetuning) predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYDkzRCcFg_1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryi9WodsCztH"
      },
      "outputs": [],
      "source": [
        "text = \"This is a great [MASK].\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBToj9HGFgcs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XItWdUW0GUA8"
      },
      "source": [
        "`AutoTokenizer`  automatically loads the appropriate tokenizer based on the specified pre-trained model.\n",
        "\n",
        "We'll be covering tokenizers in a later class, but in brief, tokenizers do the following\n",
        " * **Tokenization**: It breaks down text into smaller units called tokens, which could be words, subwords, or even characters, depending on the tokenizer.\n",
        " * **Vocabulary**: It has a vocabulary of known tokens and their corresponding numerical IDs.\n",
        " * **Encoding**: It converts text into numerical representations (input IDs) that can be fed into the pre-trained model.\n",
        " * **Decoding**: It converts numerical representations back into human-readable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ldvALmXCztI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRzmdToHHc0w"
      },
      "source": [
        "* Tokenize the input text:\n",
        " * Use the previously loaded tokenizer to process the text (which contains th.\n",
        "      `[MASK]` token).\n",
        " * return_tensors=\"pt\": This ensures the output is a PyTorch tensor.\n",
        " * The result `(inputs)` is a dictionary containing tokenized input IDs and other information needed by the model.\n",
        "\n",
        "* Get model predictions:\n",
        " * Pass the tokenized input to the loaded model (model).\n",
        " * `.logits`: This extracts the raw predictions (logits) from the model's output. Logits represent the model's confidence in different token possibilities.\n",
        "\n",
        "* Locate the `[MASK]` token index\n",
        "  \n",
        "* Extract logits for the `[MASK]` token\n",
        "\n",
        "* Get the top 5 most likely tokens (from `mask_token_logits`) to replace the `[MASK]`\n",
        "\n",
        "What do these completions tell us about BERT's pretrained model weights?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ4qpaiFCztJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKEIUBvkCztI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfKZQUSeMEnL"
      },
      "source": [
        "### 3. Load the IMDB Sentiment Analysis dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTcEB0j_CztJ"
      },
      "outputs": [],
      "source": [
        "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
        "    print(f\"'>>> Label: {row['label']}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r-UXY_nCztK"
      },
      "outputs": [],
      "source": [
        "tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oymEsNxCztK"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "\n",
        "# Use batched=True to activate fast multithreading!\n",
        "tokenized_datasets = imdb_dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp2DzfZBNXV6"
      },
      "source": [
        "### 4. Chunk the data\n",
        "\n",
        "Chunking  is used to handle long sequences of text more efficiently, as BERT  has limits on the input length it can process. Dictionary `chunks` will have the same keys as `concatenated_examples`, but the values will be lists of chunks instead of the original text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_9z8FLaCztK"
      },
      "outputs": [],
      "source": [
        "chunk_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl-s8GhcCztK"
      },
      "outputs": [],
      "source": [
        "# Slicing produces a list of lists for each feature\n",
        "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhuJjm1ZCztL"
      },
      "outputs": [],
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnQHiPMJCztL"
      },
      "outputs": [],
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGzFsM_eOY_U"
      },
      "source": [
        "Group and prepare text data for processing: concatenate all text examples, calculate the total length, and adjust it to ensure that the text can be divided into complete chunks of the desired chunk_size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLVhUlC7CztL"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9173Dt1yCztL"
      },
      "outputs": [],
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKStzL5ZO8qV"
      },
      "source": [
        "Decode a sequence of token IDs back into human-readable text using the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZDjfx8hCztM"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY9Y9nYhXSS0"
      },
      "source": [
        "### 5. Prepare the data for the MLM task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHLP5tlYQHTm"
      },
      "source": [
        "`DataCollatorForLanguageModeling` is designed to prepare batches of data specifically for masked language modeling (MLM) tasks.\n",
        "\n",
        "* `tokenizer`: The tokenizer to use for masking and padding the input data.\n",
        "*  `mlm_probability=0.15` This sets the probability of masking a token during data preparation. In this case, there's a 15% chance that a given token will be masked. Masking is the core principle of MLM: the model is trained to predict the masked tokens based on the surrounding context.\n",
        "* Padding: We pad sequences to the same length within a batch to ensure consistent input shapes for the model.\n",
        "* Labels: We create the \"labels\" for the MLM task, which are the original token IDs of the masked tokens. These labels are used to calculate the loss during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOjStF5kCztM"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYRA57DoCztM"
      },
      "outputs": [],
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMydu0R5CztM"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return default_data_collator(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtQvr1gECztN"
      },
      "outputs": [],
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT5pHHHpXIZ3"
      },
      "source": [
        "Create a smaller, downsampled version of the original training dataset to speed up experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNiJsclUCztN"
      },
      "outputs": [],
      "source": [
        "train_size = 10_000\n",
        "test_size = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Nca4PsYIUA"
      },
      "source": [
        "### 6. Set up MLM training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWdR8O-VCztN"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZysd5GzYSKC"
      },
      "source": [
        "Batch size and logging:\n",
        "\n",
        "* `batch_size`: Sets the batch size for training, which determines how many examples are processed at once.\n",
        "\n",
        "* `logging_steps`: Calculates how often the training loss should be logged. This is typically done after every epoch (one full pass through the training data), and it's calculated by dividing the total number of training examples by the batch size.\n",
        "\n",
        "Training arguments:\n",
        "\n",
        "* `evaluation_strategy`: Sets the frequency of evaluation during training (\"epoch\" means evaluate after each epoch).\n",
        "* `learning_rate`: Sets the learning rate for the optimizer, which controls how much the model's weights are adjusted during each training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tca6HAltCztN"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
        "    #overwrite_output_dir=True,\n",
        "    #evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT7vU0kpCztO"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    #tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np590LFlZN9b"
      },
      "source": [
        "### 7. Use the perplexity metric to determine current model quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSvAOXgWZYSh"
      },
      "source": [
        "* `evaluate()` method runs the model on the evaluation dataset and returns a dictionary containing evaluation metrics, including the evaluation loss (eval_loss).\n",
        "* Perplexity measures how well the model predicts the next token in a sequence, with lower perplexity indicating better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LknE_QFKCztO"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUIxCZE4CztO"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJ-_cV8CztO"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a57CjDfoCztO"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub() #need to do this to access the model in a later cell; make sure your HF API token has write access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETAZDw-fZvsf"
      },
      "source": [
        "### 8. Try to improve things further by performing random masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKmGBARSCztP"
      },
      "outputs": [],
      "source": [
        "def insert_random_mask(batch):\n",
        "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "    masked_inputs = data_collator(features)\n",
        "    # Create a new \"masked\" column for each column in the dataset\n",
        "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzyAjf1pCztP"
      },
      "outputs": [],
      "source": [
        "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
        "eval_dataset = downsampled_dataset[\"test\"].map(\n",
        "    insert_random_mask,\n",
        "    batched=True,\n",
        "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
        ")\n",
        "eval_dataset = eval_dataset.rename_columns(\n",
        "    {\n",
        "        \"masked_input_ids\": \"input_ids\",\n",
        "        \"masked_attention_mask\": \"attention_mask\",\n",
        "        \"masked_labels\": \"labels\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwdZqTWkCztP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(\n",
        "    downsampled_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaIWiOihbhHS"
      },
      "source": [
        "Use *Adam* optimizer. In general, an optimizer in a DL and NNs context is responsible for updating the model's parameters during training to minimize the loss function and improve the model's performance.\n",
        "\n",
        "The *AdamW* optimization algorithm, which is a variant of *Adam* that incorporates weight decay. Weight decay helps to prevent overfitting by adding a penalty to the loss function that discourages the model's parameters from becoming too large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7bbJa58CztP"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk2N9tfaCztV"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4My3PMC5ckcM"
      },
      "source": [
        "Initial hyperparameters:\n",
        "\n",
        "* `num_train_epochs`: Sets the number of training epochs to 3. An epoch is one full pass through the training data.\n",
        "* `num_update_steps_per_epoch`: Gets the number of update steps per epoch, which is determined by the length of the train_dataloader (the number of batches in the training data).\n",
        "* `num_training_steps`: Calculates the total number of training steps by multiplying the number of epochs by the number of update steps per epoch.\n",
        "\n",
        "Create a Learning Rate Scheduler:\n",
        "\n",
        "* \"linear\": Specifies the type of learning rate schedule to use. In this case, it's a linear schedule, which means the learning rate will decrease linearly from its initial value to 0 over the course of training.\n",
        "\n",
        "* `optimizer=optimizer`: Provides the optimizer that will be used during training. The scheduler will adjust the learning rate of this optimizer.\n",
        "\n",
        "* `num_warmup_steps=0`: Warmup steps are an initial period where the learning rate is gradually increased to its initial value. In this case, there's no warmup period.\n",
        "\n",
        "* `num_training_steps=num_training_steps`: Specifies the total number of training steps, which was calculated earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDOvBgsdCztV"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajIV625CCztV"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import get_full_repo_name\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfVOzUcBCztW"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "output_dir = model_name\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmpxgxedd7F_"
      },
      "source": [
        "Define the training loop:\n",
        "\n",
        "* `loss = outputs.loss`: Extracts the loss value from the model's outputs.\n",
        "* `accelerator.backward(loss)`: Performs backpropagation using the accelerator to calculate gradients of the loss with respect to the model's parameters. This is done in a distributed manner if using multiple devices.\n",
        "* `optimizer.step()`: Updates the model's parameters based on the calculated gradients using the optimizer.\n",
        "* `lr_scheduler.step()`: Updates the learning rate according to the learning rate schedule defined by the lr_scheduler.\n",
        "* `optimizer.zero_grad()`: Resets the gradients to zero before the next batch to avoid accumulating gradients from previous batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZhyFMaGCztW"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import math\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset)]\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        #repo.push_to_hub(\n",
        "        #    commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        #)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1vCRblhCztW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\n",
        "    \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRCAjJaqegMY"
      },
      "source": [
        "### 9. Check to see if the new predictions for `this is a great [MASK]` reflect the movies domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OnZnMRGCztX"
      },
      "outputs": [],
      "source": [
        "preds = mask_filler(text)\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\">>> {pred['sequence']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoCT2zjGQ3Vl"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}