{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bddf5fdf"
      },
      "source": [
        "# Class 2: Early Language Modelling & Vector Representations\n",
        "\n",
        "**Topics**\n",
        "\n",
        "* N-gram-models\n",
        "* Neural architectures\n",
        "* TF-IDF\n",
        "* Embeddings\n",
        "* Applications of embeddings\n",
        "\n",
        "**Libraries**:\n",
        "NLTK, Gensim\n",
        "\n",
        "**Reading**\n",
        "\n",
        "* [Jurafsky & Martin Chapter 3: Ngram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "* [Jurafsky & Martin Chapter 5:  Embeddings](https://web.stanford.edu/~jurafsky/slp3/5.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfb33c4"
      },
      "source": [
        "## 1. N-gram language models\n",
        "\n",
        "In the next few cells we'll illustrate the strengths and limitations of n-gram-based language models by learning a 1-5-gram language model over Jane Austen's Persuasion using probabilities first calculated via Maximum Likelihood Estimation and then converted to log probabilities\n",
        "\n",
        "We'll find that the perplexity of this language model is quite high, largely due to data sparsity -- many n-grams in the test set were not present in the training data, resulting in low or zero probabilities for these sequences in a our basic MLE model.\n",
        "\n",
        "To address this, we'll implement smoothing techniques to mitigate the sparsity problem by assigning non-zero probabilities to unseen n-grams. This will give us a significantly lowe perplexity value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933d06c6"
      },
      "source": [
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93fa8f97"
      },
      "source": [
        "## 1.1 Download and tokenize the data\n",
        "\n",
        "We'll be using the NLTK library for our language modeling task. First we'll get the raw text of *Persuasion* and perform very basic whitespace-separated tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19801134"
      },
      "source": [
        "from nltk.corpus import gutenberg\n",
        "import re\n",
        "\n",
        "# Load Persuasion\n",
        "corpus = gutenberg.raw('austen-persuasion.txt')\n",
        "\n",
        "# Tokenize the corpus by whitespace and convert to lowercase\n",
        "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
        "\n",
        "# Display the first few tokens\n",
        "print(tokens[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9fcb13a"
      },
      "source": [
        "### 1.2 Create ngrams\n",
        "\n",
        "We'll next create ngrams up to order 5--i.e,, unigrams, bigrams, etc--out of these tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "385d834a"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "ngram_dict = {}\n",
        "for n in range(1, 6):\n",
        "    ngram_dict[n] = list(ngrams(tokens, n))\n",
        "\n",
        "# Print the first few n-grams for each length\n",
        "for n, ngram_list in ngram_dict.items():\n",
        "    print(f\"{n}-grams:\")\n",
        "    print(ngram_list[:10])\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1075fe37"
      },
      "source": [
        "### 1.4 Maximum likelihood estimation\n",
        "\n",
        "(See Jurafsky & Martin sec. 3.1.2.)\n",
        "We use MLE to calculate the probabilty of an N-gram occurring based on its frequency in the training data as follows. The MLE probability of an N-gram $P(w_n | w_{n-N+1}...w_{n-1})$ is calculated as the count of the N-gram divided by the count of its context (the preceding N-1 words).\n",
        "\n",
        "For example, the probability of a bigram $P(w_i | w_{i-1})$ is calculated as:\n",
        "\n",
        "$P(w_i | w_{i-1}) = \\frac{Count(w_{i-1} w_i)}{Count(w_{i-1})}$\n",
        "\n",
        "Similarly, for a trigram $P(w_i | w_{i-2} w_{i-1})$:\n",
        "\n",
        "$P(w_i | w_{i-2} w_{i-1}) = \\frac{Count(w_{i-2} w_{i-1} w_i)}{Count(w_{i-2} w_{i-1})}$\n",
        "\n",
        "In general, for an N-gram $w_{n-N+1}...w_n$, the MLE probability of the last word $w_n$ given the preceding N-1 words is:\n",
        "\n",
        "$P(w_n | w_{n-N+1}...w_{n-1}) = \\frac{Count(w_{n-N+1}...w_n)}{Count(w_{n-N+1}...w_{n-1})}$\n",
        "\n",
        "Where:\n",
        "- $Count(w_{n-N+1}...w_n)$ is the number of times the N-gram sequence appears in the training data.\n",
        "- $Count(w_{n-N+1}...w_{n-1})$ is the number of times the context sequence (the first N-1 words of the N-gram) appears in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "000ec0d4"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "ngram_probabilities = {}\n",
        "ngram_counts = {}\n",
        "context_counts = {}\n",
        "\n",
        "for n in range(1, n + 1):\n",
        "    ngram_counts[n] = Counter(ngram_dict[n])\n",
        "    if n > 1:\n",
        "        context_counts[n] = Counter(ngram[:-1] for ngram in ngram_dict[n])\n",
        "\n",
        "for n in range(1, n + 1):\n",
        "    ngram_probabilities[n] = {}\n",
        "    for ngram, count in ngram_counts[n].items():\n",
        "        if n == 1:\n",
        "            # Probability for unigrams is count / total number of tokens\n",
        "            ngram_probabilities[n][ngram] = count / len(tokens)\n",
        "        else:\n",
        "            # Probability for n-grams (n > 1) is count of n-gram / count of context\n",
        "            context = ngram[:-1]\n",
        "            if context in context_counts[n]:\n",
        "                ngram_probabilities[n][ngram] = count / context_counts[n][context]\n",
        "            else:\n",
        "                #\n",
        "                ngram_probabilities[n][ngram] = 0\n",
        "# Display some of the calculated probabilities for each n-gram length\n",
        "for n, probabilities in ngram_probabilities.items():\n",
        "    print(f\"{n}-gram probabilities (first 10):\")\n",
        "    for ngram, prob in list(probabilities.items())[:10]:\n",
        "        print(f\"{ngram}: {prob:.6f}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "785fe33f"
      },
      "source": [
        "### 1.5 Log Probabilities\n",
        "\n",
        "As described in Jurafsky & Martin 3.1.3, multiplying many small probability values can lead to numerical underflow.  To avoid this, it is common practice to work with **log probabilities** instead of the raw probabilities. The logarithm of a product is the sum of the logarithms:\n",
        "\n",
        "$\\log(P(w_1, w_2, ..., w_m)) = \\log(\\prod_{i=1}^{m} P(w_i | w_{i-N+1}...w_{i-1}))$\n",
        "\n",
        "Using the property of logarithms, this becomes:\n",
        "\n",
        "$\\log(P(w_1, w_2, ..., w_m)) = \\sum_{i=1}^{m} \\log(P(w_i | w_{i-N+1}...w_{i-1}))$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "416cbabb"
      },
      "source": [
        "import math\n",
        "\n",
        "log_ngram_probabilities = {}\n",
        "\n",
        "for n, probabilities in ngram_probabilities.items():\n",
        "    log_ngram_probabilities[n] = {}\n",
        "    for ngram, prob in probabilities.items():\n",
        "        if prob > 0:\n",
        "            log_ngram_probabilities[n][ngram] = math.log(prob)\n",
        "        else:\n",
        "            # Assign a very small negative number for log(0) to avoid errors\n",
        "            # A common practice is to use a value related to the smallest possible non-zero probability or -infinity\n",
        "            # Here, we'll use a placeholder like -1e10 for demonstration purposes,\n",
        "            # but in a real application, more sophisticated smoothing techniques would be used.\n",
        "            log_ngram_probabilities[n][ngram] = -float('inf') # Represent log(0) as negative infinity\n",
        "\n",
        "\n",
        "# Display some of the calculated log probabilities for each n-gram length\n",
        "for n, log_probabilities in log_ngram_probabilities.items():\n",
        "    print(f\"{n}-gram log probabilities (first 10):\")\n",
        "    # Safely iterate and print to avoid errors if a dictionary is empty\n",
        "    for ngram, log_prob in list(log_probabilities.items())[:10]:\n",
        "        print(f\"{ngram}: {log_prob:.6f}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a57cf969"
      },
      "source": [
        "### 1.6 Evaluate our model using the perplexity metric.\n",
        "\n",
        "Perplexity is a measure of surprise so lower is better-- the mode is less surprised by the text.\n",
        "\n",
        "Formally, the perplexity of a language model on a test set of $m$ words $W = w_1 w_2 ... w_m$ is defined as the inverse probability of the test set, normalized by the number of words:\n",
        "\n",
        "$PP(W) = P(w_1 w_2 ... w_m)^{-\\frac{1}{m}}$\n",
        "\n",
        "Using the chain rule of probability, this can be rewritten as:\n",
        "\n",
        "$PP(W) = (\\prod_{i=1}^{m} P(w_i | w_1 ... w_{i-1}))^{-\\frac{1}{m}}$\n",
        "\n",
        "For an N-gram language model, the probability of a word $w_i$ is conditioned on the preceding N-1 words:\n",
        "\n",
        "$PP(W) = (\\prod_{i=1}^{m} P(w_i | w_{i-N+1} ... w_{i-1}))^{-\\frac{1}{m}}$\n",
        "\n",
        "To avoid numerical underflow when multiplying many small probabilities, we work with log probabilities:\n",
        "\n",
        "$\\log PP(W) = -\\frac{1}{m} \\sum_{i=1}^{m} \\log P(w_i | w_{i-N+1} ... w_{i-1})$\n",
        "\n",
        "And the perplexity is then:\n",
        "\n",
        "$PP(W) = e^{-\\frac{1}{m} \\sum_{i=1}^{m} \\log P(w_i | w_{i-N+1} ... w_{i-1})}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "244ed030"
      },
      "source": [
        "import math\n",
        "\n",
        "# 1. Define a test set of tokens\n",
        "# We'll use a portion of the existing tokens list.\n",
        "# In a real scenario, this should be a separate dataset.\n",
        "test_set_size = int(len(tokens) * 0.1) # Use 10% of the data as test set\n",
        "test_tokens = tokens[-test_set_size:]\n",
        "train_tokens = tokens[:-test_set_size] # Ensure training data is distinct\n",
        "\n",
        "# Rebuild counts and probabilities using only the training data\n",
        "# This is important for a proper evaluation on unseen data\n",
        "ngram_counts_train = {}\n",
        "context_counts_train = {}\n",
        "log_ngram_probabilities_train = {}\n",
        "\n",
        "for n in range(1, 6):\n",
        "    train_ngrams = list(ngrams(train_tokens, n))\n",
        "    ngram_counts_train[n] = Counter(train_ngrams)\n",
        "    if n > 1:\n",
        "        context_counts_train[n] = Counter(ngram[:-1] for ngram in train_ngrams)\n",
        "\n",
        "for n in range(1, 6):\n",
        "    log_ngram_probabilities_train[n] = {}\n",
        "    for ngram, count in ngram_counts_train[n].items():\n",
        "        if n == 1:\n",
        "            prob = count / len(train_tokens)\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            if context in context_counts_train[n]:\n",
        "                prob = count / context_counts_train[n][context]\n",
        "            else:\n",
        "                prob = 0\n",
        "\n",
        "        if prob > 0:\n",
        "            log_ngram_probabilities_train[n][ngram] = math.log(prob)\n",
        "        else:\n",
        "            log_ngram_probabilities_train[n][ngram] = -float('inf')\n",
        "\n",
        "\n",
        "# 2. Choose an N-gram length (e.g., N=5)\n",
        "N = 5\n",
        "\n",
        "# Generate N-grams for the test set\n",
        "test_ngrams = list(ngrams(test_tokens, N))\n",
        "\n",
        "# 3. Initialize variables\n",
        "total_log_prob = 0\n",
        "N_test = len(test_ngrams)\n",
        "\n",
        "# Handle case where test set is smaller than N\n",
        "if N_test == 0:\n",
        "    perplexity = float('inf') # Cannot calculate perplexity\n",
        "else:\n",
        "    # 4. Iterate through the test set N-grams\n",
        "    for ngram in test_ngrams:\n",
        "        # a. Get the N-gram and its context\n",
        "        # N-gram is 'ngram', context is 'ngram[:-1]' (for backoff, not strictly needed here)\n",
        "\n",
        "        # b. Look up the log probability\n",
        "        log_prob = log_ngram_probabilities_train[N].get(ngram, -float('inf')) # Use -inf for unseen ngrams\n",
        "\n",
        "        # c. Add the log probability to total\n",
        "        # Handle -inf by treating it as a very small probability (e.g., log(1e-10)) for sum\n",
        "        # This prevents total_log_prob from becoming -inf immediately\n",
        "        if log_prob == -float('inf'):\n",
        "             total_log_prob += math.log(1e-10) # Using a small value as a proxy for unseen log prob\n",
        "        else:\n",
        "            total_log_prob += log_prob\n",
        "\n",
        "    # 5. Calculate the average log probability\n",
        "    avg_log_prob = total_log_prob / N_test\n",
        "\n",
        "    # 6. Calculate perplexity\n",
        "    # Handle potential overflow/underflow for very small/large avg_log_prob\n",
        "    try:\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "    except OverflowError:\n",
        "        perplexity = float('inf')\n",
        "    except UnderflowError:\n",
        "        perplexity = 0.0 # Or a very small positive number\n",
        "\n",
        "# 7. Print the calculated perplexity value\n",
        "print(f\"Calculating perplexity for N={N} model on the test set.\")\n",
        "print(f\"Number of test N-grams: {N_test}\")\n",
        "print(f\"Total log probability: {total_log_prob:.6f}\")\n",
        "print(f\"Average log probability: {avg_log_prob:.6f}\")\n",
        "print(f\"Perplexity: {perplexity:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b19f3f1"
      },
      "source": [
        "print(f\"Calculated Perplexity for N={N} model: {perplexity:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our perplexity is quite high. Given the relatively small corpus used and  the lack of smoothing, it's not too surprising that we performed so poorly on the test data:  a significant number of N-grams in the test set  were likely not seen in the training set (sparsity), leading to low or zero probabilities for these sequences. Let's try to fix this using smoothing.\n"
      ],
      "metadata": {
        "id": "wsNWl0FB-S7e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7be24fd7"
      },
      "source": [
        "### 1.7 Fix with smoothing techniques: Add-one and Add-*k* smoothing\n",
        "\n",
        "See Jurafsky & Martin 3.3.\n",
        "Without smoothing, the probability of an unseen N-gram is 0. This is problematic because it means any sequence containing an unseen N-gram will have a total probability of 0, leading to infinite perplexity.\n",
        "\n",
        "Add-k smoothing adds a small constant $k$ to the count of every N-gram and adds $k$ times the vocabulary size to the denominator. This ensures that even unseen N-grams have a small, non-zero probability.\n",
        "\n",
        "The formula for Add-k smoothing is:\n",
        "\n",
        "$P_{Add-k}(w_n | w_{n-N+1}...w_{n-1}) = \\frac{Count(w_{n-N+1}...w_n) + k}{Count(w_{n-N+1}...w_{n-1}) + k \\times |V|}$\n",
        "\n",
        "Where:\n",
        "- $Count(w_{n-N+1}...w_n)$ is the number of times the N-gram sequence appears in the training data.\n",
        "- $Count(w_{n-N+1}...w_{n-1})$ is the number of times the context sequence (the first N-1 words of the N-gram) appears in the training data.\n",
        "- $k$ is the smoothing parameter (typically a small positive value, often between 0 and 1).\n",
        "- $|V|$ is the size of the vocabulary (the total number of unique words in the training data).\n",
        "\n",
        "When $k=1$, Add-k smoothing becomes Add-one smoothing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "# Add-one Smoothing function\n",
        "def add_one_smoothing(n, ngram_counts, context_counts, vocab_size):\n",
        "    smoothed_probabilities = {}\n",
        "    for ngram, count in ngram_counts[n].items():\n",
        "        if n == 1:\n",
        "            smoothed_probabilities[ngram] = (count + 1) / (len(train_tokens) + vocab_size)\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            # Add 1 to the count and the context count\n",
        "            smoothed_probabilities[ngram] = (count + 1) / (context_counts[n].get(context, 0) + vocab_size)\n",
        "    return smoothed_probabilities\n",
        "\n",
        "# Add-k Smoothing function\n",
        "def add_k_smoothing(n, ngram_counts, context_counts, vocab_size, k=1):\n",
        "    smoothed_probabilities = {}\n",
        "    for ngram, count in ngram_counts[n].items():\n",
        "        if n == 1:\n",
        "            smoothed_probabilities[ngram] = (count + k) / (len(train_tokens) + k * vocab_size)\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            # Add k to the count and k * vocab_size to the context count\n",
        "            smoothed_probabilities[ngram] = (count + k) / (context_counts[n].get(context, 0) + k * vocab_size)\n",
        "    return smoothed_probabilities\n",
        "\n",
        "\n",
        "# Calculate vocabulary size from the training data\n",
        "vocab = set(train_tokens)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Apply Add-one smoothing for N=5\n",
        "N_smooth = 5\n",
        "add_one_smoothed_probs = add_one_smoothing(N_smooth, ngram_counts_train, context_counts_train, vocab_size)\n",
        "\n",
        "# Apply Add-k smoothing with k=1 for N=5 (equivalent to Add-one)\n",
        "add_k_smoothed_probs_1 = add_k_smoothing(N_smooth, ngram_counts_train, context_counts_train, vocab_size, k=1)\n",
        "\n",
        "# Apply Add-k smoothing with k=0.5 for N=5\n",
        "add_k_smoothed_probs_half = add_k_smoothing(N_smooth, ngram_counts_train, context_counts_train, vocab_size, k=0.5)\n",
        "\n",
        "\n",
        "print(f\"\\nDemonstrating Smoothing for N={N_smooth}:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Example unseen N-gram from the previous perplexity calculation\n",
        "unseen_ngram = ('hall', 'in', 'its', 'national', 'importance') # Example from previous output\n",
        "\n",
        "print(f\"Example N-gram: {unseen_ngram}\")\n",
        "\n",
        "mle_prob_unseen = 0\n",
        "context_unseen = unseen_ngram[:-1]\n",
        "if context_unseen in context_counts_train[N_smooth]:\n",
        "    ngram_count_unseen = ngram_counts_train[N_smooth].get(unseen_ngram, 0)\n",
        "    context_count_unseen = context_counts_train[N_smooth][context_unseen]\n",
        "    mle_prob_unseen = ngram_count_unseen / context_count_unseen\n",
        "else:\n",
        "    mle_prob_unseen = 0 # Context not seen\n",
        "\n",
        "print(f\" Original MLE Probability (no smoothing): {mle_prob_unseen:.10f}\")\n",
        "\n",
        "# Probability with Add-one smoothing\n",
        "add_one_prob_unseen = add_one_smoothed_probs.get(unseen_ngram, (0 + 1) / (context_counts_train[N_smooth].get(context_unseen, 0) + vocab_size)) # Handle unseen ngram in smoothed dict\n",
        "print(f\"  Add-one Smoothed Probability: {add_one_prob_unseen:.10f}\")\n",
        "\n",
        "# Probability with Add-k smoothing (k=1)\n",
        "add_k_prob_unseen_1 = add_k_smoothed_probs_1.get(unseen_ngram, (0 + 1) / (context_counts_train[N_smooth].get(context_unseen, 0) + vocab_size)) # Handle unseen ngram in smoothed dict\n",
        "print(f\"  Add-k Smoothed Probability (k=1): {add_k_prob_unseen_1:.10f}\")\n",
        "\n",
        "# Probability with Add-k smoothing (k=0.5)\n",
        "add_k_prob_unseen_half = add_k_smoothed_probs_half.get(unseen_ngram, (0 + 0.5) / (context_counts_train[N_smooth].get(context_unseen, 0) + 0.5 * vocab_size)) # Handle unseen ngram in smoothed dict\n",
        "print(f\"  Add-k Smoothed Probability (k=0.5): {add_k_prob_unseen_half:.10f}\")\n"
      ],
      "metadata": {
        "id": "39EOeafFfQ4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ec0374"
      },
      "source": [
        "import math\n",
        "\n",
        "# Function to calculate perplexity with smoothed probabilities\n",
        "def calculate_smoothed_perplexity(test_ngrams, smoothed_probabilities, N, context_counts_train, vocab_size, alpha=1):\n",
        "    total_log_prob = 0\n",
        "    N_test = len(test_ngrams)\n",
        "\n",
        "    if N_test == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    for ngram in test_ngrams:\n",
        "        log_prob = -float('inf') # Default for unseen in smoothed dict\n",
        "\n",
        "        if ngram in smoothed_probabilities:\n",
        "             log_prob = math.log(smoothed_probabilities[ngram])\n",
        "        else:\n",
        "             # Calculate probability for unseen ngram in test set using smoothing formula\n",
        "             context = ngram[:-1]\n",
        "             if N == 1:\n",
        "                  prob = (0 + alpha) / (len(train_tokens) + alpha * vocab_size)\n",
        "             else:\n",
        "                  prob = (0 + alpha) / (context_counts_train[N].get(context, 0) + alpha * vocab_size)\n",
        "\n",
        "             if prob > 0:\n",
        "                 log_prob = math.log(prob)\n",
        "             else:\n",
        "                 # This case should ideally not happen with smoothing and alpha > 0\n",
        "                 log_prob = math.log(1e-10) # Fallback\n",
        "\n",
        "        total_log_prob += log_prob\n",
        "\n",
        "    avg_log_prob = total_log_prob / N_test\n",
        "\n",
        "    try:\n",
        "        perplexity = math.exp(-avg_log_prob)\n",
        "    except OverflowError:\n",
        "        perplexity = float('inf')\n",
        "    except UnderflowError:\n",
        "        perplexity = 0.0\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "print(f\"Recalculating Perplexity for N={N_smooth} with Smoothing:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Calculate perplexity with Add-one smoothing\n",
        "perplexity_add_one = calculate_smoothed_perplexity(test_ngrams, add_one_smoothed_probs, N_smooth, context_counts_train, vocab_size, alpha=1)\n",
        "print(f\"Perplexity with Add-one Smoothing: {perplexity_add_one:.4f}\")\n",
        "\n",
        "perplexity_add_k_half = calculate_smoothed_perplexity(test_ngrams, add_k_smoothed_probs_half, N_smooth, context_counts_train, vocab_size, alpha=0.5)\n",
        "print(f\"Perplexity with Add-k Smoothing (k=0.5): {perplexity_add_k_half:.4f}\")\n",
        "\n",
        "# Recall perplexity without smoothing from previous output\n",
        "perplexity_no_smoothing = perplexity\n",
        "print(f\"Perplexity without Smoothing (MLE): {perplexity_no_smoothing:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55470dac"
      },
      "source": [
        "## 2. Sparse-vector representations: TF-IDF and term-document matrices\n",
        "\n",
        "See Jurafsky & Martin 6.5.\n",
        "\n",
        "\n",
        "One of the most common techniques for creating sparse vector representations is **TF-IDF (Term Frequency-Inverse Document Frequency)**. TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It considers both the frequency of a word within a specific document (Term Frequency) and the rarity of the word across the entire corpus (Inverse Document Frequency).\n",
        "\n",
        "**Term Frequency (TF)**\n",
        "\n",
        "The Term Frequency of a word $t$ in a document $d$ is a measure of how often the word appears in that document. A simple way to calculate TF is:\n",
        "\n",
        "$TF(t, d) = \\text{Count of word } t \\text{ in document } d$\n",
        "\n",
        "However, it's often more useful to normalize the term frequency to prevent bias towards longer documents. Common normalization methods include:\n",
        "\n",
        "*   **Raw Count**: $TF(t, d) = \\text{Count}(t, d)$\n",
        "*   **Adjusted for Document Length**: $TF(t, d) = \\frac{\\text{Count}(t, d)}{\\text{Total number of words in document } d}$\n",
        "*   **Log Normalization**: $TF(t, d) = \\log(1 + \\text{Count}(t, d))$\n",
        "*   **Double Normalization**: $TF(t, d) = 0.5 + 0.5 \\times \\frac{\\text{Count}(t, d)}{\\text{Max frequency of any word in document } d}$\n",
        "\n",
        "**Inverse Document Frequency (IDF)**\n",
        "\n",
        "The Inverse Document Frequency of a word $t$ is a measure of how much information the word provides, i.e., if it's common or rare across all documents in the corpus. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the word:\n",
        "\n",
        "$IDF(t, D) = \\log(\\frac{\\text{Total number of documents } |D|}{\\text{Number of documents containing word } t})$\n",
        "\n",
        "where $|D|$ is the total number of documents in the corpus. Adding 1 to the denominator (known as \"smooth IDF\") is common to prevent division by zero for words that don't appear in any document:\n",
        "\n",
        "$IDF(t, D) = \\log(\\frac{|D|}{\\text{Number of documents containing word } t + 1}) + 1$\n",
        "\n",
        "**TF-IDF Calculation**\n",
        "\n",
        "The TF-IDF score is the product of the Term Frequency and the Inverse Document Frequency:\n",
        "\n",
        "$TF-IDF(t, d, D) = TF(t, d) \\times IDF(t, D)$\n",
        "\n",
        "A high TF-IDF score for a word in a document indicates that the word is frequent in that document but relatively rare in the rest of the corpus, making it a potentially important term for that document.\n",
        "\n",
        "**Term-Document Matrix**\n",
        "\n",
        "When we calculate the TF-IDF score for every word in the vocabulary for every document in the corpus, we can organize these scores into a **Term-Document Matrix**. In this matrix:\n",
        "\n",
        "*   Each row represents a **term** (a unique word from the vocabulary).\n",
        "*   Each column represents a **document** from the corpus.\n",
        "*   Each cell $(i, j)$ contains the **TF-IDF score** of term $i$ in document $j$.\n",
        "\n",
        "Alternatively, the matrix can be transposed, with rows representing documents and columns representing terms. This is the representation commonly produced by libraries like scikit-learn's `TfidfVectorizer`, where each row vector represents a document in the TF-IDF feature space.\n",
        "\n",
        "\n",
        "In the next few cells, we'll learn some very basic TF-IDF representations for words in a handful of open-domain books. Using these learned representations we'll encode a set of documents and calculate their cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Download Gutenberg corpus if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/gutenberg')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('gutenberg')\n",
        "except LookupError:\n",
        "    nltk.download('gutenberg')\n",
        "\n",
        "\n",
        "# Load a few texts from the Gutenberg corpus\n",
        "# Using 'austen-persuasion.txt' and 'shakespeare-hamlet.txt' as examples\n",
        "corpus_texts = [\n",
        "    gutenberg.raw('austen-persuasion.txt'),\n",
        "    gutenberg.raw('austen-emma.txt'),\n",
        "    gutenberg.raw('austen-sense.txt'),\n",
        "    gutenberg.raw('shakespeare-caesar.txt'),\n",
        "    gutenberg.raw('shakespeare-macbeth.txt'),\n",
        "    gutenberg.raw('shakespeare-hamlet.txt'),\n",
        "    gutenberg.raw('bible-kjv.txt') # Adding another text for more diversity\n",
        "]\n",
        "\n",
        "# Preprocess the texts (tokenize, lowercase, remove punctuation and numbers)\n",
        "processed_corpus = []\n",
        "for text in corpus_texts:\n",
        "    # Tokenize by word, convert to lowercase\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Join tokens back into a string for TfidfVectorizer\n",
        "    processed_corpus.append(\" \".join(tokens))\n",
        "\n",
        "print(f\"Loaded and processed {len(corpus_texts)} documents.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Train the TfidfVectorizer\n",
        "# We will treat each text as a document to build the vocabulary and calculate TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n",
        "\n",
        "# Get the vocabulary (feature names)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"TF-IDF Vectorizer trained. Vocabulary size: {len(feature_names)}\")\n",
        "print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "p_BGMvrr8bIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_to_compare = [\n",
        "    \"quick brown fox jumps dog\",\n",
        "    \"quick brown fox jumps cat\",\n",
        "    \"quick brown fox jumps dog quick brown fox jumps cat\",\n",
        "    \"I hate yellow cake\"\n",
        "]"
      ],
      "metadata": {
        "id": "kO897vbOBvC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_documents_to_compare = []\n",
        "for doc in documents_to_compare:\n",
        "    tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n",
        "    processed_documents_to_compare.append(\" \".join(tokens))\n",
        "\n",
        "# Transform the processed documents into TF-IDF vectors\n",
        "# This uses the vocabulary and IDF learned from the training corpus\n",
        "tfidf_matrix_compare = tfidf_vectorizer.transform(processed_documents_to_compare)\n",
        "\n",
        "print(\"TF-IDF Vector Representations (Sparse Matrix):\")\n",
        "print(tfidf_matrix_compare)\n",
        "print(\"-\" * 50)\n",
        "print(tfidf_matrix_compare.toarray())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"The TF-IDF vector representations of the four documents.\")\n",
        "print(\"Each row represents a document, and the columns correspond to the terms in the vocabulary learned from the training corpus.\")\n",
        "print(\"The values are the TF-IDF scores, indicating the importance of each term in each document.\")\n"
      ],
      "metadata": {
        "id": "irltbD_-Eoep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Calculate the cosine similarity matrix\n",
        "cosine_sim_matrix_compare = cosine_similarity(tfidf_matrix_compare)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(cosine_sim_matrix_compare)"
      ],
      "metadata": {
        "id": "fyLcHIZwGgYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ahS4TNM-AiVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cosine_sim_matrix_compare, annot=True, cmap='viridis', fmt=\".2f\",\n",
        "                xticklabels=[f\"Doc {i+1}\" for i in range(len(documents_to_compare))],\n",
        "                yticklabels=[f\"Doc {i+1}\" for i in range(len(documents_to_compare))])\n",
        "plt.title(\"TF-IDF Cosine Similarity Heatmap between Documents\")\n",
        "plt.xlabel(\"Document\")\n",
        "plt.ylabel(\"Document\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TkW-WNRoGM8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Neural network language models and word2vec"
      ],
      "metadata": {
        "id": "-DNV6vRIAQ60"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db30f521"
      },
      "source": [
        "**Word2Vec: Learning Dense Word Representations**\n",
        "\n",
        "See Jurafsky & Martin 6.8. While TF-IDF provides sparse vector representations of words, Word2Vec is a popular algorithm for learning **dense** vector representations of words, also known as **word embeddings**. These embeddings capture semantic relationships between words -- words with similar meanings will lie close to each other in a  continuous vector space.\n",
        "\n",
        "We will focus on the **Skip-Gram** model in this illustration.\n",
        "\n",
        "**The Skip-Gram Procedure**\n",
        "\n",
        "The Skip-Gram model works by taking pairs of words from the training text: a **target word** (the current word) and **context words** (words within a defined window around the target word). The goal is to train a model that is good at predicting context words given a target word.\n",
        "\n",
        "For each target word in the corpus, the Skip-Gram model samples context words within a specified window size. For example, with a window size of 2, for the sentence \"the quick brown fox jumps over the lazy dog\", when \"fox\" is the target word, the context words would be \"quick\", \"brown\", \"jumps\", and \"over\".\n",
        "\n",
        "The model uses a simple two-layer neural network (without a hidden layer in its original form) to learn the word embeddings.\n",
        "\n",
        "*   **Input Layer**: Takes the target word as a one-hot encoded vector.\n",
        "*   **Output Layer**: Outputs a probability distribution over the vocabulary, representing the likelihood of each word being a context word for the given target word.\n",
        "\n",
        "The core idea is that if two words have similar contexts (i.e., they appear in similar surrounding words), their learned vector representations should be close in the vector space.\n",
        "\n",
        "Word2Vec is considered a \"self-training\" or \"self-supervised\" algorithm because it learns word embeddings from the text data itself without requiring external labeled data. The training objective is derived directly from the structure of the input text.\n",
        "\n",
        "The process involves:\n",
        "\n",
        "1.  **Creating Word Pairs**: Generating (target word, context word) pairs from the training corpus based on the chosen window size.\n",
        "2.  **Training the Model**: The model is trained to maximize the probability of predicting the actual context words for a given target word. This is typically done using a technique called **Negative Sampling**. Instead of calculating the probability of all words in the vocabulary, it samples a small number of \"negative\" words (words that are *not* in the context) and trains the model to distinguish the positive context words from the negative samples.\n",
        "3.  **Learning Embeddings**: As the model trains to perform this prediction task, the weights learned in the \"projection layer\" (which effectively maps the one-hot input to a dense vector) become the word embeddings. These dense vectors capture the distributional information of words in the corpus.s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83a91f33"
      },
      "source": [
        "%pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained Word2Vec model\n",
        "# This might take some time and download a large file (around 1.5 GB for word2vec-google-news-300)\n",
        "print(\"Loading pre-trained Word2Vec model...\")\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "GzPsfhknySUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_query = \"computer\"\n",
        "similar_words = model.most_similar(word_to_query, topn=50)\n",
        "# Print the similar words and their similarity scores\n",
        "for word, similarity in similar_words:\n",
        "  print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "kHdDms7Sygm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}