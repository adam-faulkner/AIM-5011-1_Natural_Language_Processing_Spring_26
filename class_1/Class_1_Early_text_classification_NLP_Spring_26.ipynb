{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Class 1: Early text classification\n",
        "\n",
        "**Topics**\n",
        "\n",
        "* Logistic Regression\n",
        "* The bag-of-words vector representation\n",
        "\n",
        "**Reading**\n",
        "* [Jurafsky \\& Martin Chapter 5: Logistic Regression](https://web.stanford.edu/~jurafsky/slp3/5.pdf)\n"
      ],
      "metadata": {
        "id": "8F0z3dlKT2A6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c1e8b72"
      },
      "source": [
        "## 1. Multiclass classification of articles using Logistic Regression\n",
        "\n",
        "In the next few cells, we'll implement a multi-class text classification model using the 20 Newsgroups dataset -- a commonly used dataset for multiclass classification problems--and Logistic Regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a097318c"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "# Select 4 categories for a 4-class classification setting\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Inspect the dataset\n",
        "print(\"Number of training documents:\", len(newsgroups_train.data))\n",
        "print(\"Number of testing documents:\", len(newsgroups_test.data))\n",
        "print(\"Number of categories:\", len(newsgroups_train.target_names))\n",
        "print(\"Categories:\", newsgroups_train.target_names)\n",
        "print(\"\\nFirst document in training set:\")\n",
        "print(newsgroups_train.data[0])\n",
        "print(\"\\nTarget of the first document:\", newsgroups_train.target[0])\n",
        "print(\"Target name of the first document:\", newsgroups_train.target_names[newsgroups_train.target[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Per-topic distribution in dataset"
      ],
      "metadata": {
        "id": "uclQTn2I-7D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Get the category names\n",
        "category_names_train = newsgroups_train.target_names\n",
        "category_names_test = newsgroups_test.target_names\n",
        "\n",
        "# Count the occurrences of each category in the training data\n",
        "category_counts_train = np.bincount(newsgroups_train.target)\n",
        "\n",
        "# Count the occurrences of each category in the testing data\n",
        "category_counts_test = np.bincount(newsgroups_test.target)\n",
        "\n",
        "# Create a bar plot of the category distribution for the training data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=category_names_train, y=category_counts_train)\n",
        "plt.title('Distribution of News Categories in Training Data')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()\n",
        "\n",
        "# Create a bar plot of the category distribution for the test data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=category_names_test, y=category_counts_test)\n",
        "plt.title('Distribution of News Categories in Testing Data')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X87OSN75NuOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "691cc8af"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "# We can adjust parameters like max_features, stop_words, etc.\n",
        "vectorizer = CountVectorizer(max_features=3000, stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the training data and transform the training data\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Get the target variables\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Print the shape of the resulting matrices to verify\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the bag-of-words representation in our dataset. In `sklearn` this is implemented as `CountVectorizer`"
      ],
      "metadata": {
        "id": "jpTO25zWO0Iy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4bf868a"
      },
      "source": [
        "#### Bag-of-Words with sklearn's`CountVectorizer`\n",
        "\n",
        "`CountVectorizer` is a component from `scikit-learn` that plays a crucial role in converting a collection of text documents into a matrix of token counts. Essentially, it transforms text into a numerical representation, which is a necessary step before applying most machine learning algorithms.\n",
        "\n",
        "Here's how it generally works:\n",
        "\n",
        "1.  **Tokenization**: It breaks down the text into individual words or tokens. It can also handle n-grams (sequences of n words).\n",
        "2.  **Vocabulary Building**: It builds a vocabulary of all unique words found in the training documents. This vocabulary forms the columns of the resulting matrix.\n",
        "3.  **Document-Term Matrix**: For each document, it counts the occurrences of each word from the vocabulary within that document. The result is a matrix where rows represent documents and columns represent words in the vocabulary, with each cell containing the count of a specific word in a specific document.\n",
        "\n",
        "**Key parameters and their role in the current notebook:**\n",
        "\n",
        "*   `max_features=3000`: This parameter tells the vectorizer to only consider the top 3000 words (tokens) ordered by their frequency across the entire corpus. This helps in reducing the dimensionality of the feature space, preventing the matrix from becoming too large and potentially improving model performance by focusing on the most relevant words.\n",
        "*   `stop_words='english'`: This parameter instructs the vectorizer to remove common English stop words (like \"the\", \"is\", \"a\", etc.) during the tokenization process. Stop words usually carry little meaning and can clutter the feature space without providing much discriminative power.\n",
        "\n",
        "In our notebook, `CountVectorizer` was used to convert the raw text data from the 20 Newsgroups dataset into a numerical format (`X_train` and `X_test`), which then served as input for the Logistic Regression model. The output `X_train` and `X_test` are sparse matrices, meaning they primarily store non-zero counts efficiently."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"First 5 documents (count vectors) from X_train and their corresponding features:\")\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n--- Document {i+1} ---\")\n",
        "    print(\"Original Text (first 200 chars):\\n\", newsgroups_train.data[i][:200])\n",
        "\n",
        "    # Get the vector for the i-th document\n",
        "    doc_vector = X_train[i].toarray().flatten()\n",
        "\n",
        "    # Find indices of non-zero counts\n",
        "    non_zero_indices = doc_vector.nonzero()[0]\n",
        "\n",
        "    # Get the features and their counts for this document\n",
        "    features_with_counts = []\n",
        "    for idx in non_zero_indices:\n",
        "        features_with_counts.append(f\"{feature_names[idx]}: {int(doc_vector[idx])}\")\n",
        "\n",
        "    print(\"Count Vector (non-zero features):\\n\", \", \".join(features_with_counts))\n"
      ],
      "metadata": {
        "id": "SbAhlgYMOgSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Sigmoid, Softmax, and cross-entropy loss: The core components of logistic regression\n"
      ],
      "metadata": {
        "id": "ffHHhSzt_E-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.1 The sigmoid Function\n",
        "\n",
        "The sigmoid function, also known as the logistic function, is a crucial component in binary logistic regression. It's defined as:\n",
        "\n",
        "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "where $z$ is the input, typically a linear combination of features and weights: $z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$.\n",
        "\n",
        "The sigmoid function takes any real-valued input and squashes it to a value between 0 and 1. In binary logistic regression, this output is interpreted as the probability of the input belonging to the positive class (e.g., probability of a review being positive).\n",
        "\n",
        "- If $z$ is large positive, $\\sigma(z)$ approaches 1.\n",
        "- If $z$ is large negative, $\\sigma(z)$ approaches 0.\n",
        "- If $z$ is 0, $\\sigma(z)$ is 0.5.\n",
        "\n",
        "This property makes the sigmoid function ideal for modeling probabilities, as probabilities must be between 0 and 1."
      ],
      "metadata": {
        "id": "vSXqevDLKNTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2.2 The softmax function\n",
        "\n",
        "While the sigmoid function is used for binary classification, the **softmax function** is its generalization for multiclass classification. It's used to convert a vector of arbitrary real values into a probability distribution over multiple classes. The softmax function is defined as:\n",
        "\n",
        "$$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "\n",
        "where $z = (z_1, z_2, ..., z_K)$ is the input vector (the output of the linear model for each of the $K$ classes), and $z_i$ is the input for class $i$.\n",
        "\n",
        "The softmax function:\n",
        "- Takes a vector of $K$ real numbers as input.\n",
        "- Exponentiates each element to make them non-negative.\n",
        "- Divides each exponentiated element by the sum of all exponentiated elements, ensuring that the output values sum to 1.\n",
        "\n",
        "The output of the softmax function is a vector of $K$ probabilities, where each element represents the probability of the input belonging to a specific class. This makes it suitable for multiclass classification problems where we want to predict the probability of an instance belonging to each of the available classes."
      ],
      "metadata": {
        "id": "yet2lnlmv8RM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dcabd51"
      },
      "source": [
        "##### 1.2.2.1 Visualizing the Softmax Function\n",
        "\n",
        "Let's visualize this with a simple example. Suppose we have an input vector representing the scores for three classes: [1.0, 2.0, 3.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca964f6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def softmax(z):\n",
        "    \"\"\"Compute softmax scores for each class in z.\"\"\"\n",
        "    exp_z = np.exp(z - np.max(z)) # Subtract max for numerical stability\n",
        "    return exp_z / np.sum(exp_z)\n",
        "\n",
        "# Example input vector (e.g., scores for 3 classes)\n",
        "input_scores = np.array([1.0, 2.0, 3.0])\n",
        "softmax_output = softmax(input_scores)\n",
        "\n",
        "print(\"Input Scores:\", input_scores)\n",
        "print(\"Softmax Output (Probabilities):\", softmax_output)\n",
        "\n",
        "# Visualize the input scores and softmax output\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot Input Scores\n",
        "axes[0].bar(range(len(input_scores)), input_scores, color='skyblue')\n",
        "axes[0].set_title('Input Scores')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_xticks(range(len(input_scores)))\n",
        "axes[0].set_xticklabels([f'Class {i+1}' for i in range(len(input_scores))])\n",
        "axes[0].grid(axis='y', linestyle='--')\n",
        "\n",
        "# Plot Softmax Output\n",
        "axes[1].bar(range(len(softmax_output)), softmax_output, color='lightcoral')\n",
        "axes[1].set_title('Softmax Output (Probabilities)')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Probability')\n",
        "axes[1].set_xticks(range(len(softmax_output)))\n",
        "axes[1].set_xticklabels([f'Class {i+1}' for i in range(len(softmax_output))])\n",
        "axes[1].set_ylim(0, 1) # Probabilities are between 0 and 1\n",
        "axes[1].grid(axis='y', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 The cross-Entropy Loss Function\n",
        "\n",
        "The **cross-entropy loss function**, also known as logarithmic loss, is a standard loss function used in classification problems, particularly with models that output probability distributions like logistic regression and neural networks. It measures the difference between the predicted probability distribution and the true class distribution.\n",
        "\n",
        "For a single instance with $K$ classes, where $y_i$ is a binary indicator (1 if the true class is $i$, 0 otherwise) and $p_i$ is the predicted probability of the instance belonging to class $i$, the cross-entropy loss is defined as:\n",
        "\n",
        "$$ H(y, p) = -\\sum_{i=1}^{K} y_i \\log(p_i) $$\n",
        "\n",
        "In the case of a true class $j$, $y_j=1$ and $y_i=0$ for $i \\neq j$. The formula simplifies to:\n",
        "\n",
        "$$ H(y, p) = -\\log(p_j) $$\n",
        "\n",
        "The goal during training is to minimize this loss. Minimizing $-\\log(p_j)$ is equivalent to maximizing $\\log(p_j)$, which in turn is equivalent to maximizing $p_j$, the predicted probability of the true class.\n",
        "\n",
        "Cross-entropy loss penalizes confident wrong predictions heavily. If the model predicts a low probability for the true class, the $-\\log(p_j)$ term will be large, indicating a high loss. Conversely, if the model predicts a high probability for the true class, the loss will be small. This makes it an effective loss function for training classification models to output well-calibrated probabilities."
      ],
      "metadata": {
        "id": "8L97OQuDwY5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 Review of stochastic gradient descent with cross-entropy loss\n",
        "\n",
        "See Jurafsky & Martin 5.6\n",
        "\n",
        "Our goal is to find the set of weights which minimizes this loss function, averaged over all examples. Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters θ) the function’s slope is rising the most steeply, and moving in the opposite direction.\n",
        "\n",
        "The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction.\n",
        "\n",
        "For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima)\n",
        "\n",
        "Below is a simplified example of Cross-Entropy Loss and SGD.vWe assume a simple binary classification scenario with one feature (x) and we are trying to learn a single weight (w) with no bias for simplicity. The prediction is sigmoid(w * x)\n",
        "\n"
      ],
      "metadata": {
        "id": "weRFefnLAdms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# --- Simplified Example with Cross-Entropy Loss and SGD ---\n",
        "\n",
        "# Let's assume a simple binary classification scenario with one feature (x)\n",
        "# and we are trying to learn a single weight (w) with no bias for simplicity.\n",
        "# The prediction is sigmoid(w * x)\n",
        "\n",
        "# Example Data (Feature x and True Label y)\n",
        "# Data point 1: x=1, y=1 (Positive class)\n",
        "# Data point 2: x=2, y=0 (Negative class)\n",
        "X = np.array([1.0, 2.0])\n",
        "y_true = np.array([1.0, 0.0]) # True labels\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Cross-Entropy Loss function for a single data point\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # Avoid log(0) by clipping predictions\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "\n",
        "# Total Cross-Entropy Loss over all data points\n",
        "def total_cross_entropy_loss(w, X, y_true):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X)):\n",
        "        z = w * X[i]\n",
        "        y_pred = sigmoid(z)\n",
        "        total_loss += cross_entropy_loss(y_true[i], y_pred)\n",
        "    return total_loss / len(X) # Average loss\n",
        "\n",
        "# Gradient of the Cross-Entropy Loss with respect to the weight (w) for a single data point\n",
        "# For sigmoid(wx), the derivative of loss with respect to w is (y_pred - y_true) * x\n",
        "def gradient_cross_entropy(w, x, y_true):\n",
        "    y_pred = sigmoid(w * x)\n",
        "    return (y_pred - y_true) * x\n",
        "\n",
        "# Simulate Stochastic Gradient Descent\n",
        "def stochastic_gradient_descent_ce(initial_w, learning_rate, n_iterations, X, y_true):\n",
        "    w_values = [initial_w]\n",
        "    loss_values = [total_cross_entropy_loss(initial_w, X, y_true)]\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # In SGD, we pick one data point (or a mini-batch) randomly\n",
        "        # For this simple illustration, let's cycle through the data points\n",
        "        data_index = i % len(X)\n",
        "        x_i = X[data_index]\n",
        "        y_true_i = y_true[data_index]\n",
        "\n",
        "        # Calculate gradient using a single data point\n",
        "        grad = gradient_cross_entropy(w_values[-1], x_i, y_true_i)\n",
        "\n",
        "        # Update the weight\n",
        "        new_w = w_values[-1] - learning_rate * grad\n",
        "        w_values.append(new_w)\n",
        "        loss_values.append(total_cross_entropy_loss(new_w, X, y_true)) # Calculate total loss for visualization\n",
        "\n",
        "    return w_values, loss_values\n",
        "\n",
        "# Set parameters for SGD\n",
        "initial_w = -2.0 # Initial weight\n",
        "learning_rate = 0.5 # Learning rate\n",
        "n_iterations = 300 # Number of iterations\n",
        "\n",
        "# Run SGD simulation\n",
        "w_steps, loss_steps = stochastic_gradient_descent_ce(initial_w, learning_rate, n_iterations, X, y_true)\n",
        "\n",
        "# Generate w values for plotting the total loss function\n",
        "w_plot = np.linspace(-3, 4, 100)\n",
        "loss_plot = [total_cross_entropy_loss(w, X, y_true) for w in w_plot]\n",
        "\n",
        "# Create interactive plot using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add the loss function curve\n",
        "fig.add_trace(go.Scatter(x=w_plot, y=loss_plot,\n",
        "                         mode='lines',\n",
        "                         name='Average Cross-Entropy Loss'))\n",
        "\n",
        "# Add the SGD steps as points with hover information\n",
        "fig.add_trace(go.Scatter(x=w_steps, y=loss_steps,\n",
        "                         mode='markers+lines',\n",
        "                         name='SGD Steps',\n",
        "                         marker=dict(size=8),\n",
        "                         hovertemplate='Weight: %{x:.4f}<br>Loss: %{y:.4f}<extra></extra>')) # Customize hover info\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Illustration of Stochastic Gradient Descent with Cross-Entropy Loss',\n",
        "    xaxis_title='Weight Value (w)',\n",
        "    yaxis_title='Average Loss',\n",
        "    hovermode='x unified' # Show hover info for all traces at a given x-value\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0Ue4ghWlL7q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(solver='sag', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "id": "gwuZCjQAOTkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4785af67"
      },
      "source": [
        "### 1.4 Evaluating the model\n",
        " We will use common classification metrics like accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "3VPTDvtuyfs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "MBbUcqvOy9uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db99901e"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=newsgroups_test.target_names)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91743c8d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=newsgroups_test.target_names, yticklabels=newsgroups_test.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Examine the most informative per-class features\n",
        "\n",
        "To determine the features that provided the best discriminative signal, we can simply look at the model coefficients."
      ],
      "metadata": {
        "id": "4i_oGaYBDY8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Get the model coefficients\n",
        "# The coefficients are stored in the coef_ attribute of the LogisticRegression model\n",
        "# coef_ is an array of shape (n_classes, n_features)\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Get the feature names from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the class names\n",
        "class_names = newsgroups_train.target_names\n",
        "\n",
        "print(\"Most Informative Features for Each Class (Logistic Regression):\")\n",
        "\n",
        "# Iterate through each class to find the top features\n",
        "for i, class_name in enumerate(class_names):\n",
        "    # Get the coefficients for the current class\n",
        "    class_coefficients = coefficients[i]\n",
        "\n",
        "    # Create a DataFrame to associate features with their coefficients for this class\n",
        "    feature_coefficient_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'coefficient': class_coefficients\n",
        "    })\n",
        "\n",
        "    # Sort features by the absolute value of their coefficients to find the most important\n",
        "    feature_coefficient_df['abs_coefficient'] = abs(feature_coefficient_df['coefficient'])\n",
        "    most_informative_class_features = feature_coefficient_df.sort_values(by='abs_coefficient', ascending=False).head(20)\n",
        "\n",
        "    print(f\"\\n--- Class: {class_name} ---\")\n",
        "    display(most_informative_class_features[['feature', 'coefficient']])"
      ],
      "metadata": {
        "id": "RPilVwPMDJmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}