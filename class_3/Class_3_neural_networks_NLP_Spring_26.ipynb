{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBepKRq_0pya"
      },
      "source": [
        "## Class 3: Neural Networks\n",
        "\n",
        "\n",
        "In this noteboook, we'll walk through an implementation of multi-class text classification using, first simple Logistic Regression (which, you'll recall, is simply a single-neuron neural network) and then a Feed-Forward Neural Network.\n",
        "\n",
        "### Baseline model: Logistic Regression + BOW\n",
        "\n",
        "Our task is classify movie plots by genre using, first, a baseline model consisting of\n",
        "\n",
        "- Bag of words\n",
        "\n",
        "- TF-IDF\n",
        "\n",
        "### Challenger model 1: Logisic Regression + word2vec\n",
        "\n",
        "Then, we'll show how the pretrained word2vec embeeddings introduced last week can be used to improve the accuracy of this simple baseline model\n",
        "\n",
        "### Challenger Model 2: Neural-Network + word2vec\n",
        "\n",
        "Finally we'll show how a simple FFN with word2vec embeddings added to the first layer can give us our best performance on this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification task\n",
        "\n",
        "Our classification task is, given a movie title, to predict the likley genre of that movie -- *Comedy*, *Documentary*, or *Drama.*\n",
        "\n",
        "\n",
        "## Dataset\n",
        "The data for this notebook is contained in `movie_lens_1k_three_genres.csv` which should be placed in the same directory as the notebook. This is a modified version of the  MovieLens dataset."
      ],
      "metadata": {
        "id": "d8fMiUVs_L4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "WS9wMbnd2Y4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajPny8lz0pyd"
      },
      "outputs": [],
      "source": [
        "from smart_open import smart_open\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoOl3Hct0pye"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs5bAW4x0pye"
      },
      "source": [
        "## Exploring the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "balanced_genre_df = pd.read_csv(\"movie_lens_1k_three_genres.csv\")"
      ],
      "metadata": {
        "id": "FZAwVw-v3a6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5lt6HZi0pyf"
      },
      "outputs": [],
      "source": [
        "balanced_genre_df.genres.value_counts().plot(kind=\"bar\", rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL1hM4lV0pyg"
      },
      "source": [
        "Train/test split of 90/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ciuwtrzV0pyg"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(balanced_genre_df, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IAB9Zus0pyg"
      },
      "source": [
        "## Model evaluation approach\n",
        "We will use confusion matrices to evaluate all classifiers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tags = [\"Drama\", \"Comedy\", \"Documentary\"]"
      ],
      "metadata": {
        "id": "5dl3loDnnwo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tsCv_XJ0pyh"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(my_tags))\n",
        "    target_names = my_tags\n",
        "    plt.xticks(tick_marks, target_names, rotation=45)\n",
        "    plt.yticks(tick_marks, target_names)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bURZcv4H0pyh"
      },
      "outputs": [],
      "source": [
        "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
        "    print('accuracy %s' % accuracy_score(target, predictions))\n",
        "    cm = confusion_matrix(target, predictions)\n",
        "    print('confusion matrix\\n %s' % cm)\n",
        "    print('(row=expected, col=predicted)')\n",
        "\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "APIwnY-h0pyh"
      },
      "outputs": [],
      "source": [
        "def predict(vectorizer, classifier, data):\n",
        "    data_features = vectorizer.transform(data['title'])\n",
        "    predictions = classifier.predict(data_features)\n",
        "    target = data['genres']\n",
        "    evaluate_prediction(predictions, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvOWXFeT0pyh"
      },
      "source": [
        "## Baseline: bag of words, TF-IDF\n",
        "\n",
        "Let's start with some simple baselines before diving into more advanced methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8N5WYl80pyh"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIXljPMC0pyh"
      },
      "source": [
        "We remove stop-words and limit our vocabulary to 3k most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "VUqcQaFlnUi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XMxFCN_W0pyh"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbQLx4t0pyi"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "count_vectorizer = CountVectorizer(\n",
        "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
        "    preprocessor=None, stop_words='english', max_features=3000)\n",
        "train_data_features = count_vectorizer.fit_transform(train_data['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jup6BQd0pyi"
      },
      "outputs": [],
      "source": [
        "\n",
        "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(train_data_features, train_data['genres'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9QK0Cl_0pyi"
      },
      "outputs": [],
      "source": [
        "count_vectorizer.get_feature_names_out()[500:510]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phH9GECu0pyj"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "predict(count_vectorizer, logreg, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO-w39Np0pyj"
      },
      "source": [
        "36% isn't great but,as a sanity check, let's look at the most informative features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6-4bQsW0pyj"
      },
      "outputs": [],
      "source": [
        "def most_influential_words(vectorizer, genre_index=0, num_words=10):\n",
        "    features = vectorizer.get_feature_names_out()\n",
        "    max_coef = sorted(enumerate(logreg.coef_[genre_index]), key=lambda x:x[1], reverse=True)\n",
        "    return [features[x[0]] for x in max_coef[:num_words]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGibAek10pyj"
      },
      "outputs": [],
      "source": [
        "# words for the Comedy genre\n",
        "comedy_tag_id = 1\n",
        "print(my_tags[comedy_tag_id])\n",
        "most_influential_words(count_vectorizer, comedy_tag_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F52TDN9s0pyj"
      },
      "outputs": [],
      "source": [
        "train_data_features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zmgLvQ0py0"
      },
      "source": [
        "### TF-IDF\n",
        ".\n",
        "Let's modify our count-based features with TF-iDF-weights. These adjust for document length, word frequency and, most importantly for the frequency of a particular word in a particular document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m-l0UYE0py0"
      },
      "outputs": [],
      "source": [
        "tf_vect = TfidfVectorizer(\n",
        "    min_df=2, tokenizer=nltk.word_tokenize,\n",
        "    preprocessor=None, stop_words='english')\n",
        "train_data_features = tf_vect.fit_transform(train_data['title'])\n",
        "\n",
        "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(train_data_features, train_data['genres'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdlhbaEO0py0"
      },
      "outputs": [],
      "source": [
        "tf_vect.get_feature_names_out()[500:510]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsd9eEmp0py0"
      },
      "outputs": [],
      "source": [
        "predict(tf_vect, logreg, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7NZp6Pl0py1"
      },
      "source": [
        "We're doing better: 42%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czhjCjNc0py1"
      },
      "outputs": [],
      "source": [
        "most_influential_words(tf_vect, comedy_tag_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX2NBtDs0py1"
      },
      "source": [
        "# word2vec-based Logistic Regression via Averaging Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3Ydwbi40py1"
      },
      "source": [
        "Aside from their usefulness for lexical similarity tasks, word2vec-based vector representations can be used in place of BOW-based features -- this boostraps word weights with the distributional information encoded in word2vec.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained Word2Vec model\n",
        "# This might take some time and download a large file (around 1.5 GB for word2vec-google-news-300)\n",
        "print(\"Loading pre-trained Word2Vec model...\")\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "HSf5uZ107-D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P93G9F3J0py2"
      },
      "source": [
        "Example vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxKExzCo0py2"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "list(islice(model.key_to_index, 12000, 12020))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MpoyIgAi0py3"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "def word_averaging(wv, words):\n",
        "    all_words, mean = set(), []\n",
        "\n",
        "    for word in words:\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        elif word in wv.key_to_index: # Use key_to_index\n",
        "            mean.append(wv.vectors[wv.key_to_index[word]]) # Use vectors and key_to_index\n",
        "            all_words.add(wv.key_to_index[word]) # Use key_to_index\n",
        "\n",
        "    if not mean:\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,) # Use vector_size\n",
        "\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJblsLZp0py3"
      },
      "source": [
        "For word2vec we apply a different tokenization scheme. We want to preserve case as the vocabulary distingushes lower and upper case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AwKiB4a60py3"
      },
      "outputs": [],
      "source": [
        "def w2v_tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KYqDZ1Qj0py4"
      },
      "outputs": [],
      "source": [
        "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['title']), axis=1).values\n",
        "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['title']), axis=1).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbenMMm20py4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(model,test_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTdepxUm0py4"
      },
      "source": [
        "Let's see how the logistic regression classifier perform on these word-averaging document features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FzFhlqu0py5"
      },
      "outputs": [],
      "source": [
        "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train_data['genres'])\n",
        "predicted = logreg.predict(X_test_word_average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU-rtmXS0py5"
      },
      "source": [
        "47% -- a small but significant lift.  Best that we have seen so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFawkdOs0py5"
      },
      "outputs": [],
      "source": [
        "evaluate_prediction(predicted, test_data.genres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ddd812"
      },
      "source": [
        "# Feedforward Neural Network + word2vec\n",
        "Let's try to improve our basic, logistic regression-based multi-class text classification model using a simple PyTorch=based FFN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4479f2c"
      },
      "source": [
        "## Import PyTorch and Necessary Modules\n",
        "Import `torch`, `torch.nn`, `torch.optim`, and `torch.utils.data` for building and training the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcb3eeb4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "print(\"PyTorch and necessary modules imported.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53a3eaf8"
      },
      "source": [
        "genre_to_int = {genre: i for i, genre in enumerate(my_tags)}\n",
        "\n",
        "train_labels = train_data['genres'].map(genre_to_int)\n",
        "test_labels = test_data['genres'].map(genre_to_int)\n",
        "\n",
        "print(\"Genre labels converted to integers.\")\n",
        "print(\"Train labels sample:\", train_labels.head())\n",
        "print(\"Test labels sample:\", test_labels.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335ee8af"
      },
      "source": [
        "## Creating PyTorch `TensorDataset` and `DataLoader`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebe278f4"
      },
      "source": [
        "X_train_tensor = torch.tensor(X_train_word_average, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_word_average, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"PyTorch TensorDataset and DataLoader created.\")\n",
        "print(f\"Training data tensor shape: {X_train_tensor.shape}\")\n",
        "print(f\"Training labels tensor shape: {y_train_tensor.shape}\")\n",
        "print(f\"Test data tensor shape: {X_test_tensor.shape}\")\n",
        "print(f\"Test labels tensor shape: {y_test_tensor.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e91803e"
      },
      "source": [
        "## Defining a PyTorch Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3de3fd2"
      },
      "source": [
        "class GenreClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Get input dimension from X_train_word_average\n",
        "input_dim = X_train_word_average.shape[1]  # Should be 300\n",
        "hidden_dim = 128  # Example hidden layer size\n",
        "output_dim = len(my_tags)  # Number of genres, should be 3\n",
        "\n",
        "model = GenreClassifier(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "print(\"Neural Network Model Defined:\")\n",
        "print(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd728db"
      },
      "source": [
        "## Instantiate Model, Loss Function, and Optimizer\n",
        "\n",
        "Let's instntiate the previously defined `GenreClassifier` model, specify the loss function as `torch.nn.CrossEntropyLoss`, and select an optimizer, `torch.optim.Adam`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bea9040"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Criterion (loss function) and Optimizer defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deeff6c"
      },
      "source": [
        "## Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "822facc8"
      },
      "source": [
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        outputs = model(data) # Forward pass\n",
        "        loss = criterion(outputs, labels) # Calculate loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc914a3c"
      },
      "source": [
        "## Evaluating the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9a48876"
      },
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "predictions_list = []\n",
        "true_labels_list = []\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    for data, labels in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1) # Get the index of the max log-probability as the prediction\n",
        "        predictions_list.extend(predicted.cpu().numpy()) # Collect predictions\n",
        "        true_labels_list.extend(labels.cpu().numpy()) # Collect true labels\n",
        "\n",
        "# Convert integer predictions and true labels back to genre names\n",
        "int_to_genre = {i: genre for i, genre in enumerate(my_tags)}\n",
        "predicted_genre_names = [int_to_genre[p] for p in predictions_list]\n",
        "true_genre_names = [int_to_genre[t] for t in true_labels_list]\n",
        "\n",
        "# Evaluate the model using the provided function\n",
        "print(\"Neural Network Model Evaluation:\")\n",
        "evaluate_prediction(predicted_genre_names, true_genre_names, title=\"NN Classifier Confusion Matrix\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! 50% accuracy achieved with a very simple architecture and no regularization or hyperparameter tuning."
      ],
      "metadata": {
        "id": "h5Eg110-N1sX"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}